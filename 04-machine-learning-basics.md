# ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ (Machine Learning Basics)

## í•™ìŠµ ëª©í‘œ

ì´ ë¬¸ì„œì—ì„œëŠ” PyTorchë¥¼ í™œìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…ê³¼ ì‹¤ìŠµì„ ë‹¤ë£¬ë‹¤.

1. **PyTorch ê¸°ì´ˆ**: í…ì„œ ì—°ì‚°, ìë™ ë¯¸ë¶„, ì£¼ìš” ëª¨ë“ˆ ì´í•´
2. **ì„ í˜• íšŒê·€**: ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•œ ê°€ì¤‘ì¹˜ ìµœì í™” ì›ë¦¬ í•™ìŠµ
3. **ë¡œì§€ìŠ¤í‹± íšŒê·€**: ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ í•´ê²°
4. **ì‹ ê²½ë§ êµ¬ì¡°**: ANN, RNN, LSTM êµ¬í˜„ ë° ì‘ìš©

---

## 1. PyTorch ê¸°ì´ˆ

### 1.1 PyTorch ì†Œê°œ

PyTorchëŠ” Facebook AI Research(FAIR)ì—ì„œ ê°œë°œí•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ, NumPyì™€ ìœ ì‚¬í•œ í…ì„œ ì—°ì‚°ê³¼ ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

**ì£¼ìš” ëª¨ë“ˆ:**
- `torch`: í…ì„œ ì—°ì‚° ë° ìˆ˜í•™ í•¨ìˆ˜
- `torch.autograd`: ìë™ ë¯¸ë¶„ (ì—­ì „íŒŒ)
- `torch.nn`: ì‹ ê²½ë§ ë ˆì´ì–´, í™œì„±í™” í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜
- `torch.optim`: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (SGD, Adam ë“±)

**ì„¤ì¹˜:**
```bash
# CPU ë²„ì „
pip install torch torchvision

# CUDA 11.8 (NVIDIA GPU)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

> **PyTorch vs TensorFlow**
>
> **PyTorch ì¥ì :**
> - ì§ê´€ì ì¸ Python ë¬¸ë²• (ë™ì  ê·¸ë˜í”„)
> - ë””ë²„ê¹…ì´ ì‰¬ì›€ (ì¼ë°˜ Python ë””ë²„ê±° ì‚¬ìš© ê°€ëŠ¥)
> - ì—°êµ¬ ë° í”„ë¡œí† íƒ€ì´í•‘ì— ìœ ë¦¬
>
> **TensorFlow ì¥ì :**
> - í”„ë¡œë•ì…˜ ë°°í¬ì— ê°•í•¨ (TensorFlow Serving, TFLite)
> - Google ìƒíƒœê³„ì™€ì˜ í†µí•© (TPU, Colab)
>
> **ìµœê·¼ íŠ¸ë Œë“œ:**
> - í•™ê³„: PyTorch ì••ë„ì  ìš°ì„¸
> - ì‚°ì—…ê³„: ë‘ í”„ë ˆì„ì›Œí¬ ëª¨ë‘ í™œë°œíˆ ì‚¬ìš©

### 1.2 í…ì„œ (Tensor)

**í…ì„œì˜ ì°¨ì›:**
- **Scalar (0D)**: ë‹¨ì¼ ìˆ«ì (ì˜ˆ: ì˜¨ë„ 25Â°C)
- **Vector (1D)**: ìˆ«ì ë°°ì—´ (ì˜ˆ: [1, 2, 3])
- **Matrix (2D)**: 2ì°¨ì› ë°°ì—´ (ì˜ˆ: ì´ë¯¸ì§€ì˜ í•œ ì±„ë„)
- **Tensor (3D+)**: 3ì°¨ì› ì´ìƒ (ì˜ˆ: RGB ì´ë¯¸ì§€ëŠ” 3D í…ì„œ)

```python
import torch
import numpy as np

# í…ì„œ ìƒì„±
t1 = torch.FloatTensor([1, 2, 3, 4])
t2 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)

# NumPy ë°°ì—´ì—ì„œ ë³€í™˜
arr = np.array([1, 2, 3])
t3 = torch.from_numpy(arr)

# ê¸°ë³¸ ì†ì„±
print(t2.shape)       # torch.Size([2, 2])
print(t2.dtype)       # torch.float32
print(t2.device)      # cpu (ë˜ëŠ” cuda:0)
```

### 1.3 í…ì„œ ì—°ì‚°

**ê¸°ë³¸ ì—°ì‚°:**
```python
import torch

# ì‚°ìˆ  ì—°ì‚°
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(a + b)          # tensor([5., 7., 9.])
print(a * b)          # ì›ì†Œë³„ ê³±ì…ˆ
print(torch.dot(a, b))  # ë‚´ì : 32.0

# ë¸Œë¡œë“œìºìŠ¤íŒ… (ìë™ í¬ê¸° ë§ì¶¤)
m1 = torch.FloatTensor([[1, 2]])  # (1, 2)
m2 = torch.FloatTensor([[3], [4]])  # (2, 1)
print(m1 + m2)
# tensor([[4., 5.],
#         [5., 6.]])
```

**í–‰ë ¬ ê³±ì…ˆ:**
```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])  # (2, 2)
m2 = torch.FloatTensor([[1], [2]])         # (2, 1)

# í–‰ë ¬ê³± (Matrix Multiplication)
print(torch.matmul(m1, m2))  # (2, 1)
# tensor([[ 5.],
#         [11.]])

# @ ì—°ì‚°ì (Python 3.5+)
print(m1 @ m2)  # ë™ì¼
```

> **ì›ì†Œë³„ ê³± vs í–‰ë ¬ê³±**
>
> ```python
> a = torch.tensor([[1, 2], [3, 4]])
> b = torch.tensor([[2, 0], [1, 2]])
>
> # ì›ì†Œë³„ ê³± (Element-wise)
> print(a * b)
> # tensor([[2, 0],
> #         [3, 8]])
>
> # í–‰ë ¬ê³± (Matrix Multiplication)
> print(a @ b)
> # tensor([[ 4,  4],
> #         [10,  8]])
> ```
>
> **ì£¼ì˜:** ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ ì—°ì‚°ì€ ëŒ€ë¶€ë¶„ í–‰ë ¬ê³±(`@` ë˜ëŠ” `matmul`)ì´ë‹¤.

**í†µê³„ í•¨ìˆ˜:**
```python
t = torch.FloatTensor([[1, 2], [3, 4]])

print(t.mean())        # 2.5 (ì „ì²´ í‰ê· )
print(t.mean(dim=0))   # tensor([2., 3.]) (ì—´ ê¸°ì¤€)
print(t.mean(dim=1))   # tensor([1.5, 3.5]) (í–‰ ê¸°ì¤€)

print(t.sum())         # 10.0
print(t.max())         # 4.0
print(t.argmax())      # 3 (ìµœëŒ“ê°’ì˜ ì¸ë±ìŠ¤)
```

**í˜•íƒœ ë³€í™˜:**
```python
t = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])  # (3, 2)

# view: í˜•íƒœ ë³€ê²½ (ë©”ëª¨ë¦¬ ê³µìœ )
print(t.view(2, 3))    # (2, 3)
print(t.view(-1, 1))   # (6, 1) - -1ì€ ìë™ ê³„ì‚°

# squeeze/unsqueeze: ì°¨ì› ì œê±°/ì¶”ê°€
t2 = torch.FloatTensor([1, 2, 3])  # (3,)
print(t2.unsqueeze(0))  # (1, 3) - í–‰ ë²¡í„°
print(t2.unsqueeze(1))  # (3, 1) - ì—´ ë²¡í„°

t3 = torch.FloatTensor([[1], [2]])  # (2, 1)
print(t3.squeeze())     # tensor([1., 2.]) - (2,)
```

---

## 2. ì„ í˜• íšŒê·€ (Linear Regression)

### 2.1 ì„ í˜• íšŒê·€ ê°œë…

ì„ í˜• íšŒê·€ëŠ” ì…ë ¥ ë°ì´í„°(x)ì™€ ì¶œë ¥ ë°ì´í„°(y) ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

**ê°€ì„¤ (Hypothesis):**
```
H(x) = Wx + b
```

**ë¹„ìš© í•¨ìˆ˜ (Cost Function):**
```
MSE = (1/n) Î£ (y - H(x))Â²
```

**ìµœì í™”:**
- ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ìœ¼ë¡œ Wì™€ bë¥¼ ì—…ë°ì´íŠ¸
- í•™ìŠµë¥ (learning rate)ë¡œ ì—…ë°ì´íŠ¸ í¬ê¸° ì¡°ì ˆ

> **í•™ìŠµë¥ (Learning Rate) ì„¤ì • íŒ**
>
> **í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´:**
> - ì†ì‹¤ì´ ë°œì‚°í•˜ê±°ë‚˜ ì§„ë™
> - ìµœì ê°’ì„ ì§€ë‚˜ì³ë²„ë¦¼
>
> **í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìœ¼ë©´:**
> - í•™ìŠµì´ ë§¤ìš° ëŠë¦¼
> - ì§€ì—­ ìµœì†Ÿê°’(local minimum)ì— ë¹ ì§ˆ ìœ„í—˜
>
> **ì¼ë°˜ì ì¸ ì‹œì‘ê°’:**
> - ì„ í˜• íšŒê·€: 0.01 ~ 0.1
> - ì‹ ê²½ë§: 0.001 ~ 0.01
> - Adam ì˜µí‹°ë§ˆì´ì €: 0.001 (ê¸°ë³¸ê°’)
>
> **ì‹¤ë¬´ íŒ:**
> - í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© (ì ì§„ì  ê°ì†Œ)
> - ì²˜ìŒì—” í° ê°’ìœ¼ë¡œ ì‹œì‘í•´ì„œ ì ì  ì¤„ì´ê¸°

### 2.2 ìˆ˜ë™ êµ¬í˜„ (ì €ìˆ˜ì¤€)

```python
import torch
import torch.optim as optim

# í›ˆë ¨ ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (requires_grad=Trueë¡œ ìë™ ë¯¸ë¶„ í™œì„±í™”)
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = optim.SGD([W, b], lr=0.01)

# í•™ìŠµ ë£¨í”„
nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    # ê°€ì„¤ (ì˜ˆì¸¡)
    hypothesis = x_train * W + b

    # ë¹„ìš© ê³„ì‚° (MSE)
    cost = torch.mean((hypothesis - y_train) ** 2)

    # ê²½ì‚¬í•˜ê°•ë²•
    optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
    cost.backward()        # ì—­ì „íŒŒ (ìë™ ë¯¸ë¶„)
    optimizer.step()       # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

    # ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        print(f"Epoch {epoch:4d}/{nb_epochs} | Cost: {cost.item():.6f} | W: {W.item():.3f}, b: {b.item():.3f}")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Epoch    0/1000 | Cost: 18.666666 | W: 0.187, b: 0.080
Epoch  100/1000 | Cost: 0.000011 | W: 1.999, b: 0.004
Epoch  200/1000 | Cost: 0.000000 | W: 2.000, b: 0.000
...
```

### 2.3 nn.Module í™œìš© (ê³ ìˆ˜ì¤€)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# ëª¨ë¸ ì •ì˜
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)  # ì…ë ¥ 1ê°œ, ì¶œë ¥ 1ê°œ

    def forward(self, x):
        return self.linear(x)

# ë°ì´í„°
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

# ëª¨ë¸ ìƒì„±
model = LinearRegressionModel()

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# í•™ìŠµ
for epoch in range(1000):
    # ì˜ˆì¸¡
    prediction = model(x_train)

    # ì†ì‹¤ ê³„ì‚°
    loss = criterion(prediction, y_train)

    # ì—­ì „íŒŒ ë° ìµœì í™”
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch {epoch:4d} | Loss: {loss.item():.6f}")

# í•™ìŠµëœ ê°€ì¤‘ì¹˜ í™•ì¸
print(f"\nW: {model.linear.weight.item():.3f}")
print(f"b: {model.linear.bias.item():.3f}")
```

### 2.4 ë‹¤ë³€ëŸ‰ ì„ í˜• íšŒê·€

ì…ë ¥ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ë°ì´í„° (x1, x2, x3) â†’ y
x_train = torch.FloatTensor([
    [73, 80, 75],
    [93, 88, 93],
    [89, 91, 90],
    [96, 98, 100],
    [73, 66, 70]
])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# ëª¨ë¸
model = nn.Linear(3, 1)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=1e-5)

# í•™ìŠµ
for epoch in range(2000):
    prediction = model(x_train)
    loss = criterion(prediction, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 200 == 0:
        print(f"Epoch {epoch:4d} | Loss: {loss.item():.2f}")

# ìƒˆ ë°ì´í„° ì˜ˆì¸¡
new_data = torch.FloatTensor([[80, 85, 90]])
pred = model(new_data)
print(f"\nì˜ˆì¸¡ê°’: {pred.item():.2f}")
```

---

## 3. ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)

### 3.1 ë¡œì§€ìŠ¤í‹± íšŒê·€ ê°œë…

**ì´ì§„ ë¶„ë¥˜**ë¥¼ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜. ì„ í˜• íšŒê·€ì˜ ì¶œë ¥ì— **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**ë¥¼ ì ìš©í•˜ì—¬ 0~1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜í•œë‹¤.

**ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜:**
```
Ïƒ(z) = 1 / (1 + e^(-z))
```

**ê°€ì„¤:**
```
H(x) = Ïƒ(Wx + b)
```

**ì†ì‹¤ í•¨ìˆ˜:**
- Binary Cross-Entropy Loss
```
BCE = -[yÂ·log(H(x)) + (1-y)Â·log(1-H(x))]
```

### 3.2 ë¡œì§€ìŠ¤í‹± íšŒê·€ êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# ë°ì´í„° (x1, x2) â†’ y (0 ë˜ëŠ” 1)
x_train = torch.FloatTensor([
    [1, 2],
    [2, 3],
    [3, 1],
    [4, 3],
    [5, 3],
    [6, 2]
])
y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])

# ëª¨ë¸
class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        return self.sigmoid(self.linear(x))

model = BinaryClassifier()

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# í•™ìŠµ
for epoch in range(1000):
    prediction = model(x_train)
    loss = criterion(prediction, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch {epoch:4d} | Loss: {loss.item():.4f}")

# ì˜ˆì¸¡
with torch.no_grad():
    test_data = torch.FloatTensor([[2.5, 2.0]])
    pred = model(test_data)
    print(f"\nì˜ˆì¸¡ í™•ë¥ : {pred.item():.4f}")
    print(f"í´ë˜ìŠ¤: {1 if pred >= 0.5 else 0}")
```

> **ğŸ’¡ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ**
>
> | ë¬¸ì œ ìœ í˜• | ì†ì‹¤ í•¨ìˆ˜ | PyTorch í•¨ìˆ˜ |
> |-----------|-----------|--------------|
> | íšŒê·€ (ì—°ì†ê°’ ì˜ˆì¸¡) | MSE, MAE | `nn.MSELoss()`, `nn.L1Loss()` |
> | ì´ì§„ ë¶„ë¥˜ | Binary Cross-Entropy | `nn.BCELoss()` |
> | ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ | Cross-Entropy | `nn.CrossEntropyLoss()` |
>
> **ì£¼ì˜:**
> - `BCELoss`ëŠ” ì¶œë ¥ì´ 0~1 ì‚¬ì´ì—¬ì•¼ í•¨ (ì‹œê·¸ëª¨ì´ë“œ ì ìš© í•„ìš”)
> - `CrossEntropyLoss`ëŠ” ë‚´ë¶€ì— Softmax í¬í•¨ (ë”°ë¡œ ì ìš©í•˜ë©´ ì•ˆ ë¨)

---

## 4. ì¸ê³µì‹ ê²½ë§ (Artificial Neural Network)

### 4.1 í¼ì…‰íŠ¸ë¡  (Perceptron)

**ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ :**
- AND, OR ê²Œì´íŠ¸ëŠ” êµ¬í˜„ ê°€ëŠ¥
- XOR ê²Œì´íŠ¸ëŠ” ë¶ˆê°€ëŠ¥ (ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€)

**ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP):**
- ì€ë‹‰ì¸µ(Hidden Layer)ì„ ì¶”ê°€í•˜ì—¬ ë¹„ì„ í˜• ë¬¸ì œ í•´ê²°
- í™œì„±í™” í•¨ìˆ˜ë¡œ ë¹„ì„ í˜•ì„± ì¶”ê°€

### 4.2 í™œì„±í™” í•¨ìˆ˜

**ì£¼ìš” í™œì„±í™” í•¨ìˆ˜:**
```python
import torch
import torch.nn as nn

x = torch.linspace(-5, 5, 100)

# ReLU (Rectified Linear Unit)
relu = nn.ReLU()
y_relu = relu(x)
# f(x) = max(0, x)

# Sigmoid
sigmoid = nn.Sigmoid()
y_sigmoid = sigmoid(x)
# f(x) = 1 / (1 + e^(-x))

# Tanh
tanh = nn.Tanh()
y_tanh = tanh(x)
# f(x) = (e^x - e^(-x)) / (e^x + e^(-x))

# Leaky ReLU
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
y_leaky = leaky_relu(x)
# f(x) = x if x > 0 else 0.01 * x
```

> **í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ**
>
> **ReLU (ê°€ì¥ ì¼ë°˜ì ):**
> - ì¥ì : ê³„ì‚° ë¹ ë¦„, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì™„í™”
> - ë‹¨ì : Dying ReLU (ìŒìˆ˜ì—ì„œ ê¸°ìš¸ê¸° 0)
> - ì‚¬ìš©: ì€ë‹‰ì¸µ ëŒ€ë¶€ë¶„
>
> **Leaky ReLU:**
> - ReLUì˜ Dying ë¬¸ì œ í•´ê²°
> - ìŒìˆ˜ì—ì„œë„ ì‘ì€ ê¸°ìš¸ê¸° ìœ ì§€
>
> **Sigmoid:**
> - ì¶œë ¥: 0~1 (í™•ë¥  í•´ì„ ê°€ëŠ¥)
> - ì‚¬ìš©: ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ
> - ë‹¨ì : ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ
>
> **Tanh:**
> - ì¶œë ¥: -1~1
> - Sigmoidë³´ë‹¤ ì¤‘ì‹¬ì´ 0ì— ê°€ê¹Œì›Œ í•™ìŠµ ì•ˆì •
> - ë‹¨ì : ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ
>
> **Softmax:**
> - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì¶œë ¥ì¸µ
> - ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥ ì˜ í•© = 1

### 4.3 MNIST ì†ê¸€ì”¨ ë¶„ë¥˜

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# ë°ì´í„° ë¡œë”©
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    download=True
)
test_dataset = datasets.MNIST(
    root='./data',
    train=False,
    transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# ëª¨ë¸ ì •ì˜ (28x28 = 784 ì…ë ¥)
class MNISTClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)  # 10ê°œ í´ë˜ìŠ¤
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 784)  # í‰íƒ„í™”
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # SoftmaxëŠ” CrossEntropyLossì— í¬í•¨
        return x

model = MNISTClassifier()

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ
nb_epochs = 5
for epoch in range(nb_epochs):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f"Epoch {epoch+1}/{nb_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}")

# í‰ê°€
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        _, predicted = torch.max(output, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()

accuracy = 100 * correct / total
print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.2f}%")
```

> **ğŸ’¡ ê³¼ì í•©(Overfitting) ë°©ì§€ ê¸°ë²•**
>
> **1. Dropout:**
> ```python
> class Model(nn.Module):
>     def __init__(self):
>         super().__init__()
>         self.fc1 = nn.Linear(784, 128)
>         self.dropout = nn.Dropout(0.5)  # 50% ë‰´ëŸ° ë¹„í™œì„±í™”
>         self.fc2 = nn.Linear(128, 10)
>
>     def forward(self, x):
>         x = F.relu(self.fc1(x))
>         x = self.dropout(x)  # í•™ìŠµ ì‹œì—ë§Œ ì ìš©
>         return self.fc2(x)
> ```
>
> **2. Batch Normalization:**
> - ê° ë ˆì´ì–´ì˜ ì…ë ¥ì„ ì •ê·œí™”
> - í•™ìŠµ ì•ˆì •í™” ë° ì†ë„ í–¥ìƒ
>
> **3. Early Stopping:**
> - ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ë©´ í•™ìŠµ ì¤‘ë‹¨
>
> **4. Data Augmentation:**
> - ì´ë¯¸ì§€: íšŒì „, í¬ë¡­, í”Œë¦½ ë“±
> - í…ìŠ¤íŠ¸: ë™ì˜ì–´ ì¹˜í™˜, ì—­ë²ˆì—­ ë“±

---

## 5. ìˆœí™˜ì‹ ê²½ë§ (RNN)

### 5.1 RNN ê°œë…

**RNN (Recurrent Neural Network)**ì€ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹ ê²½ë§ìœ¼ë¡œ, ì´ì „ ë‹¨ê³„ì˜ ì€ë‹‰ ìƒíƒœë¥¼ í˜„ì¬ ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

**ì‚¬ìš© ì‚¬ë¡€:**
- ìì—°ì–´ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê¸°ê³„ ë²ˆì—­)
- ì‹œê³„ì—´ ì˜ˆì¸¡ (ì£¼ê°€, ë‚ ì”¨)
- ìŒì„± ì¸ì‹

**RNN êµ¬ì¡°:**
```
h_t = tanh(W_hh * h_(t-1) + W_xh * x_t + b)
```

### 5.2 RNN êµ¬í˜„

```python
import torch
import torch.nn as nn

# RNN ë ˆì´ì–´
rnn = nn.RNN(
    input_size=10,   # ì…ë ¥ íŠ¹ì„± ìˆ˜
    hidden_size=20,  # ì€ë‹‰ ìƒíƒœ í¬ê¸°
    num_layers=2,    # RNN ë ˆì´ì–´ ìˆ˜
    batch_first=True # (batch, seq, feature) ìˆœì„œ
)

# ì…ë ¥: (batch_size=3, seq_len=5, input_size=10)
input_data = torch.randn(3, 5, 10)

# ì´ˆê¸° ì€ë‹‰ ìƒíƒœ: (num_layers=2, batch_size=3, hidden_size=20)
h0 = torch.zeros(2, 3, 20)

# ìˆœì „íŒŒ
output, hn = rnn(input_data, h0)

print(output.shape)  # (3, 5, 20) - ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì˜ ì¶œë ¥
print(hn.shape)      # (2, 3, 20) - ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ
```

**ê°„ë‹¨í•œ ë¬¸ì ì˜ˆì¸¡ ëª¨ë¸:**
```python
import torch
import torch.nn as nn
import torch.optim as optim

# ë°ì´í„°: "hello" â†’ "ello"
char_set = ['h', 'e', 'l', 'o']
char_to_idx = {c: i for i, c in enumerate(char_set)}
idx_to_char = {i: c for i, c in enumerate(char_set)}

# ì…ë ¥/ì¶œë ¥
input_seq = [char_to_idx[c] for c in "hell"]
target_seq = [char_to_idx[c] for c in "ello"]

# ì›-í•« ì¸ì½”ë”©
input_one_hot = [torch.eye(4)[i] for i in input_seq]
input_tensor = torch.stack(input_one_hot).unsqueeze(0)  # (1, 4, 4)
target_tensor = torch.LongTensor(target_seq)

# ëª¨ë¸
class CharRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out)
        return out.view(-1, out.size(2))

model = CharRNN(input_size=4, hidden_size=8, output_size=4)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# í•™ìŠµ
for epoch in range(100):
    output = model(input_tensor)
    loss = criterion(output, target_tensor)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0:
        print(f"Epoch {epoch:3d} | Loss: {loss.item():.4f}")

# ì˜ˆì¸¡
with torch.no_grad():
    output = model(input_tensor)
    _, predicted = torch.max(output, 1)
    predicted_chars = ''.join([idx_to_char[i.item()] for i in predicted])
    print(f"\nì…ë ¥: hell â†’ ì˜ˆì¸¡: {predicted_chars}")
```

### 5.3 LSTM (Long Short-Term Memory)

**LSTMì˜ ì¥ì :**
- RNNì˜ **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ** í•´ê²°
- ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥ (ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬)

**LSTM ê²Œì´íŠ¸:**
1. **Forget Gate**: ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìŠì„ì§€ ê²°ì •
2. **Input Gate**: ìƒˆ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°›ì•„ë“¤ì¼ì§€ ê²°ì •
3. **Output Gate**: í˜„ì¬ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ì¶œë ¥í• ì§€ ê²°ì •

```python
import torch
import torch.nn as nn

# LSTM ë ˆì´ì–´
lstm = nn.LSTM(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True
)

# ì…ë ¥
input_data = torch.randn(3, 5, 10)

# ì´ˆê¸° ìƒíƒœ (ì€ë‹‰ ìƒíƒœ + ì…€ ìƒíƒœ)
h0 = torch.zeros(2, 3, 20)
c0 = torch.zeros(2, 3, 20)

# ìˆœì „íŒŒ
output, (hn, cn) = lstm(input_data, (h0, c0))

print(output.shape)  # (3, 5, 20)
print(hn.shape)      # (2, 3, 20)
print(cn.shape)      # (2, 3, 20)
```

**LSTM ë¬¸ì ì˜ˆì¸¡:**
```python
class CharLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out)
        return out.view(-1, out.size(2))

# ì‚¬ìš©ë²•ì€ CharRNNê³¼ ë™ì¼
model = CharLSTM(input_size=4, hidden_size=8, output_size=4)
```

> **RNN vs LSTM vs GRU**
>
> | ëª¨ë¸ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‚¬ë¡€ |
> |------|------|------|-----------|
> | **RNN** | ê°„ë‹¨, ë¹ ë¦„ | ê¸°ìš¸ê¸° ì†Œì‹¤, ì¥ê¸° ì˜ì¡´ì„± ì•½í•¨ | ì§§ì€ ì‹œí€€ìŠ¤ |
> | **LSTM** | ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥ | íŒŒë¼ë¯¸í„° ë§ìŒ, ëŠë¦¼ | ê¸´ ì‹œí€€ìŠ¤, ë³µì¡í•œ íŒ¨í„´ |
> | **GRU** | LSTMë³´ë‹¤ ë¹ ë¦„, ì„±ëŠ¥ ìœ ì‚¬ | LSTMë³´ë‹¤ ì•½ê°„ ë‚®ì€ í‘œí˜„ë ¥ | ì¼ë°˜ì ì¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ |
>
> **ì‹¤ë¬´ íŒ:**
> - ë¨¼ì € LSTMìœ¼ë¡œ ì‹œì‘
> - ì†ë„ê°€ ì¤‘ìš”í•˜ë©´ GRU ê³ ë ¤
> - ìµœê·¼ì—ëŠ” **Transformer**ê°€ RNN ê³„ì—´ì„ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸ (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)

---

## 6. ì‹¤ìŠµ í”„ë¡œì íŠ¸

### 6.1 ë°°ì¶” ê°€ê²© ì˜ˆì¸¡ (ì„ í˜• íšŒê·€)

**ë°ì´í„° í˜•ì‹ (cabbage.csv):**
```
í‰ê· ê¸°ì˜¨(Â°C),ê°•ìˆ˜ëŸ‰(mm),ê°€ê²©(ì›/kg)
5.2,10.5,3200
7.8,25.3,2800
12.3,5.1,2500
...
```

```python
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv("cabbage.csv", encoding="utf-8")
X = df[['í‰ê· ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)']].values
y = df['ê°€ê²©(ì›/kg)'].values.reshape(-1, 1)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# í…ì„œ ë³€í™˜
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test)

# ëª¨ë¸
model = nn.Linear(2, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# í•™ìŠµ
for epoch in range(2000):
    prediction = model(X_train)
    loss = criterion(prediction, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 200 == 0:
        print(f"Epoch {epoch:4d} | Loss: {loss.item():.2f}")

# í‰ê°€
with torch.no_grad():
    test_pred = model(X_test)
    test_loss = criterion(test_pred, y_test)
    print(f"\ní…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss.item():.2f}")

# ìƒˆ ë°ì´í„° ì˜ˆì¸¡
new_data = torch.FloatTensor([[10.0, 15.0]])  # 10Â°C, 15mm
pred = model(new_data)
print(f"ì˜ˆì¸¡ ê°€ê²©: {pred.item():.0f}ì›/kg")
```

### 6.2 Flask ì„œë²„ë¡œ ë°°í¬

```python
from flask import Flask, request, jsonify
import torch
import torch.nn as nn

app = Flask(__name__)

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model = nn.Linear(2, 1)
model.load_state_dict(torch.load("cabbage_model.pth"))
model.eval()

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json
    temp = data["temperature"]
    rainfall = data["rainfall"]

    # ì˜ˆì¸¡
    input_tensor = torch.FloatTensor([[temp, rainfall]])
    with torch.no_grad():
        prediction = model(input_tensor)

    return jsonify({
        "temperature": temp,
        "rainfall": rainfall,
        "predicted_price": round(prediction.item(), 2)
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
```

**ì‚¬ìš©ë²•:**
```bash
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), "cabbage_model.pth")

# ì„œë²„ ì‹¤í–‰
python app.py

# API í˜¸ì¶œ
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"temperature": 10, "rainfall": 15}'
```
