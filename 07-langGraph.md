# LangGraph

## í•™ìŠµ ëª©í‘œ

ì´ ë¬¸ì„œì—ì„œëŠ” LangGraphë¥¼ í™œìš©í•œ ê³ ê¸‰ AI ì›Œí¬í”Œë¡œìš° ì„¤ê³„ì™€ êµ¬í˜„ì„ ë‹¤ë£¬ë‹¤.

1. LangGraph ê°œìš”: RAGì˜ í•œê³„ì™€ ê·¸ë˜í”„ ê¸°ë°˜ ì ‘ê·¼ë²•
2. í•µì‹¬ ê°œë…: State, Node, Edge, Conditional Edgeì˜ ì´í•´
3. Self-RAG: ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ë¡œ Hallucination ê°ì†Œ
4. Corrective RAG: ì§ˆë¬¸ ì¬ì‘ì„±ê³¼ ë°˜ë³µ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
5. ì‹¤ì „ ì‘ìš©: ì›¹ ê²€ìƒ‰ í†µí•© RAG ì‹œìŠ¤í…œ êµ¬ì¶•

## ì „ì²´ ë¡œë“œë§µ

LangGraph í•™ìŠµì€ ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰ëœë‹¤:

1ë‹¨ê³„: RAGì˜ ë¬¸ì œì  ì´í•´
- ê³ ì •ëœ íŒŒì´í”„ë¼ì¸ â†’ ìœ ì—°ì„± ë¶€ì¡±
- Hallucination â†’ ê²€ì¦ ë¶ˆê°€
- ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ëŒ€ì•ˆ ì—†ìŒ

2ë‹¨ê³„: LangGraph ê¸°ë³¸ ê°œë…
- State â†’ Node â†’ Edge
- Conditional Edge
- ìˆœí™˜ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°

3ë‹¨ê³„: ê³ ê¸‰ RAG íŒ¨í„´
- Self-RAG: ê´€ë ¨ì„± í‰ê°€
- Corrective RAG: ì§ˆë¬¸ ì¬ì‘ì„±
- Web Search RAG: ì™¸ë¶€ ê²€ìƒ‰ í†µí•©

---

## 1. LangGraphë€ ë¬´ì—‡ì¸ê°€?

### 1.1 LangGraph ê°œìš”

LangGraphëŠ” LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íë¦„ì„ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ëª¨ë¸ë§í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë‹¤. LangChain ìƒíƒœê³„ì˜ ì¼ë¶€ë¡œ, ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ Node, Edge, Stateë¡œ ì„¤ê³„í•˜ì—¬ ìˆœí™˜ì ì´ê³  ìœ ì—°í•œ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

ì„¤ì¹˜:
```bash
pip install langgraph langchain langchain-openai
```

> ìš©ì–´ ì •ë¦¬: ê·¸ë˜í”„ (Graph)
>
> ì»´í“¨í„° ê³¼í•™ì—ì„œ ê·¸ë˜í”„ëŠ” ë…¸ë“œ(Node)ì™€ ì—£ì§€(Edge)ë¡œ êµ¬ì„±ëœ ìë£Œêµ¬ì¡°ë‹¤:
>
> - Node (ë…¸ë“œ): ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¨ìœ„ (ì˜ˆ: ë¬¸ì„œ ê²€ìƒ‰, ë‹µë³€ ìƒì„±)
> - Edge (ì—£ì§€): ë…¸ë“œ ê°„ ì—°ê²° (ì˜ˆ: ê²€ìƒ‰ â†’ ë‹µë³€)
> - Directed Graph (ë°©í–¥ ê·¸ë˜í”„): ì—£ì§€ì— ë°©í–¥ì´ ìˆìŒ
> - Cyclic Graph (ìˆœí™˜ ê·¸ë˜í”„): ë…¸ë“œë¡œ ë‹¤ì‹œ ëŒì•„ì˜¬ ìˆ˜ ìˆìŒ
>
> LangGraphì˜ íŠ¹ì§•:
> - ë°©í–¥ ê·¸ë˜í”„ (Directed)
> - ìˆœí™˜ ê°€ëŠ¥ (Cyclic) â† í•µì‹¬
> - ì¡°ê±´ë¶€ ë¶„ê¸° (Conditional)

### 1.2 LangGraphê°€ í•„ìš”í•œ ì´ìœ 

LangChainì˜ ê¸°ë³¸ ì²´ì¸ êµ¬ì¡°ëŠ” ë‹¨ë°©í–¥ íŒŒì´í”„ë¼ì¸ì´ë‹¤:

```
Document Loader â†’ Text Splitter â†’ Embedding â†’ VectorStore
                                                â†“
                                            Retriever
                                                â†“
                                    Prompt â†’ Model â†’ Answer
```

ë¬¸ì œì :

| ë¬¸ì œ | ì„¤ëª… | ì˜í–¥ |
|------|------|------|
| ê³ ì •ëœ íë¦„ | í•œ ë²ˆ ì‹¤í–‰ í›„ ìˆ˜ì • ë¶ˆê°€ | ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ëŒ€ì‘ ë¶ˆê°€ |
| í‰ê°€ ë¶ˆê°€ | ì¤‘ê°„ ê²°ê³¼ ê²€ì¦ ì—†ìŒ | Hallucination ë°©ì§€ ì–´ë ¤ì›€ |
| ì¡°ê±´ë¶€ ì²˜ë¦¬ ë¶€ì¬ | if-else ë¡œì§ êµ¬í˜„ ì–´ë ¤ì›€ | ìƒí™©ë³„ ëŒ€ì‘ ë¶ˆê°€ |
| ì¬ì‹œë„ ë¶ˆê°€ | ì‹¤íŒ¨ ì‹œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘ | ë¹„íš¨ìœ¨ì  |

> ğŸ’¡ ì‹¤ë¬´ ê´€ì : ê¸°ë³¸ RAGì˜ í•œê³„
>
> í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê¸°ë³¸ RAGë¥¼ ìš´ì˜í•˜ë©´ì„œ ê²ªëŠ” ì‹¤ì œ ë¬¸ì œë“¤:
>
> ì‚¬ë¡€ 1: ê²€ìƒ‰ ì‹¤íŒ¨
> - ì§ˆë¬¸: "2023ë…„ íšŒì‚¬ ë§¤ì¶œì€?"
> - ë¬¸ì„œì—ëŠ” "ì‘ë…„ ë§¤ì¶œ 258ì¡°ì›" (2023ì´ë¼ëŠ” í‚¤ì›Œë“œ ì—†ìŒ)
> - ê²°ê³¼: ê²€ìƒ‰ ì‹¤íŒ¨ â†’ "ì •ë³´ ì—†ìŒ" ë‹µë³€
>
> ì‚¬ë¡€ 2: ì˜ëª»ëœ ê²€ìƒ‰
> - ì§ˆë¬¸: "íŒŒì´ì¬ ë²„ì „ì€?"
> - ê²€ìƒ‰: "íŒŒì´ì¬" í‚¤ì›Œë“œë¡œ ë¬´ê´€í•œ ë¬¸ì„œ ê²€ìƒ‰
> - ê²°ê³¼: ì˜ëª»ëœ ì •ë³´ ê¸°ë°˜ ë‹µë³€ (Hallucination)
>
> ì‚¬ë¡€ 3: ë¶ˆì™„ì „í•œ ì •ë³´
> - ì§ˆë¬¸: "ìµœì‹  AI íŠ¸ë Œë“œëŠ”?"
> - ë‚´ë¶€ ë¬¸ì„œ: 2023ë…„ê¹Œì§€ë§Œ ìˆìŒ
> - ê²°ê³¼: êµ¬ì‹ ì •ë³´ ì œê³µ (ì›¹ ê²€ìƒ‰ í•„ìš”)
>
> LangGraph ë„ì… í›„:
> - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì§ˆë¬¸ ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰
> - ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± í‰ê°€ í›„ ì¬ì‹œë„
> - ë‚´ë¶€ ë¬¸ì„œ ë¶€ì¡± ì‹œ ì›¹ ê²€ìƒ‰ ìë™ í†µí•©
>
> ì‹¤ì œ ì •í™•ë„ê°€ 65% â†’ 85%ë¡œ í–¥ìƒ (ë‚´ë¶€ í…ŒìŠ¤íŠ¸ ê²°ê³¼)

### 1.3 LangGraph vs LangChain Agent

| êµ¬ë¶„ | LangChain Agent | LangGraph |
|------|-----------------|-----------|
| ì œì–´ ë°©ì‹ | LLMì´ ììœ¨ ê²°ì • | ê°œë°œìê°€ ëª…ì‹œì  ì •ì˜ |
| ì˜ˆì¸¡ ê°€ëŠ¥ì„± | ë‚®ìŒ (LLM íŒë‹¨ì— ì˜ì¡´) | ë†’ìŒ (ê³ ì •ëœ ë¡œì§) |
| ë¹„ìš© | ë†’ìŒ (ë°˜ë³µì  LLM í˜¸ì¶œ) | ë‚®ìŒ (í•„ìš” ì‹œë§Œ í˜¸ì¶œ) |
| ë””ë²„ê¹… | ì–´ë ¤ì›€ | ì‰¬ì›€ (ê° ë…¸ë“œ ì¶”ì  ê°€ëŠ¥) |
| ì‚¬ìš© ì‚¬ë¡€ | íƒìƒ‰ì  ì‘ì—…, ì‹¤í—˜ | ì •í˜•í™”ëœ ì›Œí¬í”Œë¡œìš° |

ì˜ˆì‹œ: ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ëŒ€ì‘

```python
# LangChain Agent (LLMì´ íŒë‹¨)
# â†’ ì˜ˆì¸¡ ë¶ˆê°€, ì—‰ëš±í•œ ë„êµ¬ ì„ íƒ ê°€ëŠ¥

# LangGraph (ëª…ì‹œì  ì •ì˜)
def evaluate_documents(state):
    if state["relevance_score"] < 0.5:
        return "rewrite_query"  # ê°œë°œìê°€ ì •ì˜í•œ ë¡œì§
    return "generate_answer"
```

> ê°œì¸ ì˜ê²¬: ì–¸ì œ Agentë¥¼ ì“°ê³  ì–¸ì œ LangGraphë¥¼ ì“¸ê¹Œ
>
> LangChain Agent ì‚¬ìš©:
> - ì‘ì—…ì´ ë¹„ì •í˜•ì ì´ê³  íƒìƒ‰ì ì¼ ë•Œ
> - ì˜ˆ: "ì¸í„°ë„·ì—ì„œ ì •ë³´ ì°¾ê³  ìš”ì•½í•´ì¤˜" (ë²”ìœ„ê°€ ë„“ìŒ)
> - ì‹¤íŒ¨í•´ë„ ê´œì°®ì€ ê²½ìš° (ì‹¤í—˜, ë‚´ë¶€ ë„êµ¬)
>
> LangGraph ì‚¬ìš©:
> - ì›Œí¬í”Œë¡œìš°ê°€ ëª…í™•í•  ë•Œ
> - ì˜ˆ: "ë¬¸ì„œ ê²€ìƒ‰ â†’ í‰ê°€ â†’ ì¬ê²€ìƒ‰ or ë‹µë³€" (ë‹¨ê³„ê°€ ì •ì˜ë¨)
> - ì‹ ë¢°ì„±ì´ ì¤‘ìš”í•œ ê²½ìš° (ê³ ê° ëŒ€ì‘, í”„ë¡œë•ì…˜)
>
> ì‹¤ë¬´ì—ì„œëŠ” 80%ëŠ” LangGraph, 20%ëŠ” Agentë¥¼ ì‚¬ìš©í•œë‹¤. AgentëŠ” "ë³´í—˜"ìœ¼ë¡œ ìƒê°í•˜ì.

---

## 2. LangGraph ê°œë…

### 2.1 State (ìƒíƒœ)

StateëŠ” ë…¸ë“œ ê°„ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ê³µìœ  ë©”ëª¨ë¦¬ë‹¤. Pythonì˜ `TypedDict`ë¡œ ì •ì˜í•˜ë©°, ê° ë…¸ë“œëŠ” Stateë¥¼ ì½ê³  ì—…ë°ì´íŠ¸í•œë‹¤.

State ì •ì˜:
```python
from typing import TypedDict, Annotated
import operator

class RAGState(TypedDict):
    question: str              # ì‚¬ìš©ì ì§ˆë¬¸
    documents: list[str]       # ê²€ìƒ‰ëœ ë¬¸ì„œ
    answer: str                # ìƒì„±ëœ ë‹µë³€
    relevance_score: float     # ê´€ë ¨ì„± ì ìˆ˜
    retry_count: int           # ì¬ì‹œë„ íšŸìˆ˜
```

State ì—…ë°ì´íŠ¸ ë°©ì‹:

| ë°©ì‹ | ì„¤ëª… | ì½”ë“œ ì˜ˆì‹œ |
|------|------|-----------|
| Overwrite (ê¸°ë³¸) | ê°’ì„ ë®ì–´ì”€ | `state["answer"] = "ìƒˆ ë‹µë³€"` |
| Append (ì¶”ê°€) | ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ | `Annotated[list, operator.add]` |
| Merge (ë³‘í•©) | ë”•ì…”ë„ˆë¦¬ ë³‘í•© | `Annotated[dict, merge_dict]` |

Append ì˜ˆì‹œ:
```python
from typing import Annotated
import operator

class State(TypedDict):
    # ê¸°ë³¸: ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ë®ì–´ì”€
    question: str

    # operator.add: ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
    documents: Annotated[list[str], operator.add]

# ì‚¬ìš© ì˜ˆì‹œ
# Node 1: documents = ["ë¬¸ì„œ1"]
# Node 2: documents = ["ë¬¸ì„œ2"]
# ìµœì¢… State: documents = ["ë¬¸ì„œ1", "ë¬¸ì„œ2"]
```

> âš ï¸ ì£¼ì˜ì‚¬í•­: State í¬ê¸° ê´€ë¦¬
>
> StateëŠ” ë§¤ ë…¸ë“œë§ˆë‹¤ ë³µì‚¬ë˜ë¯€ë¡œ í¬ê¸°ê°€ ì»¤ì§€ë©´ ì„±ëŠ¥ ì €í•˜:
>
> ë¬¸ì œ:
> ```python
> class State(TypedDict):
>     documents: Annotated[list[str], operator.add]  # ê³„ì† ëˆ„ì 
>
> # 10ê°œ ë…¸ë“œ ê±°ì¹˜ë©´ ë¬¸ì„œê°€ 100ê°œ ì´ìƒ ëˆ„ì  ê°€ëŠ¥
> ```
>
> í•´ê²°:
> 1. í•„ìš”í•œ ì •ë³´ë§Œ ì €ì¥
>    ```python
>    state["top_documents"] = documents[:3]  # ìƒìœ„ 3ê°œë§Œ
>    ```
>
> 2. ì¤‘ê°„ ê²°ê³¼ ì‚­ì œ
>    ```python
>    def cleanup_node(state):
>        return {"documents": []}  # ì´ˆê¸°í™”
>    ```
>
> 3. ì™¸ë¶€ ì €ì¥ì†Œ ì‚¬ìš©
>    ```python
>    # Stateì—ëŠ” IDë§Œ ì €ì¥
>    state["document_ids"] = ["doc1", "doc2"]
>    # ì‹¤ì œ ë‚´ìš©ì€ DBë‚˜ ìºì‹œì— ì €ì¥
>    ```

### 2.2 Node (ë…¸ë“œ)

NodeëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë‹¤. Stateë¥¼ ì…ë ¥ë°›ì•„ ì²˜ë¦¬ í›„ ì—…ë°ì´íŠ¸ëœ Stateë¥¼ ë°˜í™˜í•œë‹¤.

Node êµ¬ì¡°:
```python
def node_function(state: State) -> State:
    # 1. Stateì—ì„œ í•„ìš”í•œ ì •ë³´ ì½ê¸°
    question = state["question"]

    # 2. ì‘ì—… ìˆ˜í–‰ (DB ì¡°íšŒ, API í˜¸ì¶œ, LLM ì‹¤í–‰ ë“±)
    result = perform_task(question)

    # 3. State ì—…ë°ì´íŠ¸
    return {"answer": result}
```

ì‹¤ì œ ì˜ˆì‹œ: ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

def retrieve_documents(state: RAGState) -> RAGState:
    """ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    question = state["question"]

    # ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰
    vectorstore = FAISS.load_local("./db", OpenAIEmbeddings())
    docs = vectorstore.similarity_search(question, k=3)

    # State ì—…ë°ì´íŠ¸
    return {
        "documents": [doc.page_content for doc in docs]
    }
```

> Node ì„¤ê³„ ì›ì¹™
>
> 1. ë‹¨ì¼ ì±…ì„ (Single Responsibility)
> - ê° ë…¸ë“œëŠ” í•˜ë‚˜ì˜ ëª…í™•í•œ ì‘ì—…ë§Œ ìˆ˜í–‰
> - ë‚˜ìœ ì˜ˆ: `retrieve_and_answer_node` (ê²€ìƒ‰ + ë‹µë³€)
> - ì¢‹ì€ ì˜ˆ: `retrieve_node`, `answer_node` (ë¶„ë¦¬)
>
> 2. ìˆœìˆ˜ í•¨ìˆ˜ ì§€í–¥
> - ì™¸ë¶€ ìƒíƒœ ë³€ê²½ ìµœì†Œí™”
> - ê°™ì€ ì…ë ¥ì´ë©´ ê°™ì€ ì¶œë ¥
>
> 3. ì—ëŸ¬ ì²˜ë¦¬
> ```python
> def safe_retrieve_node(state):
>     try:
>         docs = vectorstore.search(state["question"])
>         return {"documents": docs}
>     except Exception as e:
>         return {"documents": [], "error": str(e)}
> ```
>
> 4. ë¡œê¹…
> ```python
> import logging
>
> def retrieve_node(state):
>     logging.info(f"Retrieving for: {state['question']}")
>     docs = vectorstore.search(state["question"])
>     logging.info(f"Found {len(docs)} documents")
>     return {"documents": docs}
> ```

### 2.3 Edge (ì—£ì§€)

EdgeëŠ” ë…¸ë“œ ê°„ ì—°ê²°ì„ ì •ì˜í•œë‹¤. ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œë¥¼ ì§€ì •í•œë‹¤.

ê¸°ë³¸ Edge:
```python
from langgraph.graph import StateGraph, END

graph = StateGraph(RAGState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_documents)
graph.add_node("answer", generate_answer)

# Edge ì¶”ê°€ (retrieve â†’ answer)
graph.add_edge("retrieve", "answer")

# ë§ˆì§€ë§‰ ë…¸ë“œ â†’ END
graph.add_edge("answer", END)
```

íë¦„:
```
retrieve â†’ answer â†’ END
```

### 2.4 Conditional Edge (ì¡°ê±´ë¶€ ì—£ì§€)

Conditional EdgeëŠ” ì¡°ê±´ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ì„ íƒí•œë‹¤. if-else ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

êµ¬ì¡°:
```python
def condition_function(state: State) -> str:
    """ì¡°ê±´ íŒë‹¨ í•¨ìˆ˜"""
    if state["relevance_score"] > 0.7:
        return "generate_answer"  # ë…¸ë“œ ì´ë¦„ ë°˜í™˜
    else:
        return "rewrite_query"

# Conditional Edge ì¶”ê°€
graph.add_conditional_edges(
    "evaluate",              # í˜„ì¬ ë…¸ë“œ
    condition_function,      # ì¡°ê±´ í•¨ìˆ˜
    {
        "generate_answer": "answer",    # ì¡°ê±´ ê²°ê³¼ â†’ ë…¸ë“œ ë§¤í•‘
        "rewrite_query": "rewrite"
    }
)
```

íë¦„:
```
evaluate â†’ (ì¡°ê±´ íŒë‹¨)
            â”œâ”€ relevance_score > 0.7 â†’ answer
            â””â”€ relevance_score â‰¤ 0.7 â†’ rewrite
```

ì‹¤ì œ ì˜ˆì‹œ:
```python
def evaluate_relevance(state: RAGState) -> str:
    """ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    relevance_score = state["relevance_score"]
    retry_count = state.get("retry_count", 0)

    if relevance_score > 0.7:
        return "good"
    elif retry_count >= 3:
        return "give_up"  # 3ë²ˆ ì¬ì‹œë„ í›„ í¬ê¸°
    else:
        return "retry"

graph.add_conditional_edges(
    "evaluate",
    evaluate_relevance,
    {
        "good": "generate_answer",
        "retry": "rewrite_query",
        "give_up": END
    }
)
```

> ìˆœí™˜ ê·¸ë˜í”„ (Cyclic Graph)
>
> LangGraphì˜ í•µì‹¬ ê°•ì ì€ ìˆœí™˜ì´ë‹¤:
>
> ê¸°ì¡´ LangChain (Acyclic):
> ```
> retrieve â†’ answer â†’ END
> (í•œ ë²ˆ ì§€ë‚˜ê°€ë©´ ë)
> ```
>
> LangGraph (Cyclic):
> ```
> retrieve â†’ evaluate â†’ (ê´€ë ¨ì„± ë‚®ìŒ) â†’ rewrite â†’ retrieve
>     â†“                      â†‘
>  (ê´€ë ¨ì„± ë†’ìŒ)              â””â”€â”€â”€ ìˆœí™˜ ê°€ëŠ¥!
>     â†“
>  answer â†’ END
> ```
>
> ì¥ì :
> - ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
> - í’ˆì§ˆ ë³´ì¥ (ê¸°ì¤€ ì¶©ì¡±ê¹Œì§€ ë°˜ë³µ)
> - ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ëª¨ë¸ë§
>
> ì£¼ì˜:
> - ë¬´í•œ ë£¨í”„ ë°©ì§€ í•„ìˆ˜ (`retry_count` ë“±)
> - `recursion_limit` ì„¤ì • ê¶Œì¥

---

## 3. LangGraph êµ¬í˜„ ì‹¤ìŠµ

### 3.1 ê·¸ë˜í”„ ìƒì„± ê¸°ë³¸

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

# 1. State ì •ì˜
class RAGState(TypedDict):
    question: str
    answer: str

# 2. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def question_node(state: RAGState) -> RAGState:
    return {"question": state["question"]}

def answer_node(state: RAGState) -> RAGState:
    answer = f"{state['question']}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤."
    return {"answer": answer}

# 3. ê·¸ë˜í”„ ìƒì„±
graph = StateGraph(RAGState)

# 4. ë…¸ë“œ ì¶”ê°€
graph.add_node("question", question_node)
graph.add_node("answer", answer_node)

# 5. Edge ì¶”ê°€
graph.set_entry_point("question")  # ì‹œì‘ ë…¸ë“œ
graph.add_edge("question", "answer")
graph.add_edge("answer", END)

# 6. ì»´íŒŒì¼
app = graph.compile()

# 7. ì‹¤í–‰
result = app.invoke({"question": "LangGraphë€?"})
print(result["answer"])
```

### 3.2 ê·¸ë˜í”„ ì‹œê°í™”

```python
from IPython.display import Image, display

# Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
display(Image(app.get_graph().draw_mermaid_png()))
```

ì¶œë ¥:
```
question â†’ answer â†’ END
```

> ğŸ’¡ ì‹¤ë¬´ íŒ: ë””ë²„ê¹… ë„êµ¬
>
> 1. ìƒì„¸ ë¡œê·¸ ì¶œë ¥:
> ```python
> result = app.invoke(
>     {"question": "LangGraphë€?"},
>     config={"recursion_limit": 10}
> )
>
> # ê° ë…¸ë“œ ì‹¤í–‰ ê³¼ì • í™•ì¸
> for step in app.stream({"question": "..."}):
>     print(step)
> ```
>
> 2. Checkpointerë¡œ ìƒíƒœ ì¶”ì :
> ```python
> from langgraph.checkpoint.memory import MemorySaver
>
> memory = MemorySaver()
> app = graph.compile(checkpointer=memory)
>
> # thread_idë¡œ ì„¸ì…˜ ê´€ë¦¬
> config = {"configurable": {"thread_id": "user-123"}}
> result = app.invoke({"question": "..."}, config=config)
>
> # ì´ì „ ìƒíƒœ ì¡°íšŒ ê°€ëŠ¥
> ```
>
> 3. LangSmith ì—°ë™ (ìœ ë£Œ):
> - ëª¨ë“  ë…¸ë“œ ì‹¤í–‰ ê¸°ë¡ ìë™ ì €ì¥
> - ì›¹ UIë¡œ ì‹œê°í™”
> - ì„±ëŠ¥ ë¶„ì„ ë° ë¹„ìš© ì¶”ì 

### 3.3 ì‹¤í–‰ ì˜µì…˜

```python
# ê¸°ë³¸ ì‹¤í–‰
result = app.invoke({"question": "..."})

# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ë‹¨ê³„ë³„ ê²°ê³¼)
for step in app.stream({"question": "..."}):
    print(step)

# ì¬ê·€ ì œí•œ ì„¤ì •
result = app.invoke(
    {"question": "..."},
    config={"recursion_limit": 10}  # ìµœëŒ€ 10ë²ˆ ìˆœí™˜
)
```

---

## 4. Self-RAG: ê´€ë ¨ì„± í‰ê°€ë¡œ Hallucination ê°ì†Œ

### 4.1 Self-RAG ê°œë…

Self-RAGëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬, ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ ì‚¬ìš©ì„ ë°©ì§€í•œë‹¤.

ê¸°ì¡´ RAG ë¬¸ì œ:
```
ì§ˆë¬¸: "LangGraphì˜ ì¥ì ì€?"
ê²€ìƒ‰: ["LangChain ì†Œê°œ", "Python ê¸°ì´ˆ", "RAG ê°œìš”"]
       â†‘ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ í¬í•¨

LLM: ê´€ë ¨ ì—†ëŠ” ì •ë³´ê¹Œì§€ ì„ì–´ì„œ ë‹µë³€ â†’ Hallucination ë°œìƒ
```

Self-RAG í•´ê²°:
```
ì§ˆë¬¸: "LangGraphì˜ ì¥ì ì€?"
ê²€ìƒ‰: ["LangChain ì†Œê°œ", "Python ê¸°ì´ˆ", "RAG ê°œìš”"]
       â†“
í‰ê°€: ê° ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
       â”œâ”€ "LangChain ì†Œê°œ": 0.3 (ë‚®ìŒ) â†’ ì œê±°
       â”œâ”€ "Python ê¸°ì´ˆ": 0.1 (ë‚®ìŒ) â†’ ì œê±°
       â””â”€ "RAG ê°œìš”": 0.8 (ë†’ìŒ) â†’ ì‚¬ìš©

LLM: ê´€ë ¨ ë†’ì€ ë¬¸ì„œë§Œ ì‚¬ìš© â†’ ì •í™•í•œ ë‹µë³€
```

### 4.2 Self-RAG êµ¬í˜„

State ì •ì˜:
```python
from typing import TypedDict

class SelfRAGState(TypedDict):
    question: str
    documents: list[str]
    relevance_scores: list[float]
    filtered_documents: list[str]
    answer: str
```

ë…¸ë“œ ì •ì˜:

1. ê²€ìƒ‰ ë…¸ë“œ:
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

def retrieve_node(state: SelfRAGState) -> SelfRAGState:
    """ë¬¸ì„œ ê²€ìƒ‰"""
    vectorstore = FAISS.load_local("./db", OpenAIEmbeddings())
    docs = vectorstore.similarity_search(state["question"], k=5)

    return {
        "documents": [doc.page_content for doc in docs]
    }
```

2. ê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ:
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

def evaluate_relevance_node(state: SelfRAGState) -> SelfRAGState:
    """ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€"""
    question = state["question"]
    documents = state["documents"]

    # LLMìœ¼ë¡œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ 0~1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}
ë¬¸ì„œ: {document}

ì ìˆ˜ë§Œ ìˆ«ìë¡œ ì¶œë ¥í•˜ì„¸ìš” (ì˜ˆ: 0.8)"""
    )

    scores = []
    for doc in documents:
        result = llm.invoke(prompt.format(question=question, document=doc))
        score = float(result.content.strip())
        scores.append(score)

    # ì ìˆ˜ 0.5 ì´ìƒë§Œ í•„í„°ë§
    filtered_docs = [
        doc for doc, score in zip(documents, scores)
        if score >= 0.5
    ]

    return {
        "relevance_scores": scores,
        "filtered_documents": filtered_docs
    }
```

3. ë‹µë³€ ìƒì„± ë…¸ë“œ:
```python
def generate_answer_node(state: SelfRAGState) -> SelfRAGState:
    """í•„í„°ë§ëœ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€"""
    question = state["question"]
    documents = state["filtered_documents"]

    if not documents:
        return {"answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    context = "\n\n".join(documents)

    llm = ChatOpenAI(model="gpt-4o-mini")
    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""
    )

    result = llm.invoke(prompt.format(context=context, question=question))

    return {"answer": result.content}
```

ê·¸ë˜í”„ êµ¬ì„±:
```python
from langgraph.graph import StateGraph, END

# ê·¸ë˜í”„ ìƒì„±
graph = StateGraph(SelfRAGState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_node)
graph.add_node("evaluate", evaluate_relevance_node)
graph.add_node("answer", generate_answer_node)

# Edge ì¶”ê°€
graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "evaluate")
graph.add_edge("evaluate", "answer")
graph.add_edge("answer", END)

# ì»´íŒŒì¼
app = graph.compile()

# ì‹¤í–‰
result = app.invoke({"question": "LangGraphì˜ ì¥ì ì€?"})
print(result["answer"])
```

íë¦„ë„:
```
retrieve â†’ evaluate â†’ answer â†’ END
           (ê´€ë ¨ì„± í‰ê°€)
```

> ğŸ’¡ ì‹¤ë¬´ ê´€ì : ê´€ë ¨ì„± í‰ê°€ ìµœì í™”
>
> LLMìœ¼ë¡œ ê° ë¬¸ì„œë¥¼ í‰ê°€í•˜ë©´ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§ì´ ë“ ë‹¤:
>
> ë¬¸ì œ:
> - ë¬¸ì„œ 10ê°œ Ã— LLM í˜¸ì¶œ = 10ë²ˆ API í˜¸ì¶œ
> - GPT-4 ì‚¬ìš© ì‹œ $0.03/1K í† í° Ã— 10 = $0.3+ (í•œ ë²ˆì—!)
>
> ìµœì í™” ë°©ë²•:
>
> 1. ë°°ì¹˜ ì²˜ë¦¬ (Batch Processing):
> ```python
> # í•œ ë²ˆì— ëª¨ë“  ë¬¸ì„œ í‰ê°€
> prompt = """ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„±ì„ ê°ê° 0~1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”:
>
> ì§ˆë¬¸: {question}
>
> ë¬¸ì„œ 1: {doc1}
> ë¬¸ì„œ 2: {doc2}
> ...
>
> JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥: [0.8, 0.3, 0.9, ...]
> """
> ```
>
> 2. Reranker ëª¨ë¸ ì‚¬ìš©:
> ```python
> from sentence_transformers import CrossEncoder
>
> # ë¬´ë£Œ ë¡œì»¬ ëª¨ë¸ (ë¹ ë¥´ê³  ì €ë ´)
> reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
>
> # ì ìˆ˜ ê³„ì‚°
> scores = reranker.predict([
>     (question, doc) for doc in documents
> ])
> ```
>
> 3. Cohere Rerank API (ìœ ë£Œ, ì •í™•ë„ ë†’ìŒ):
> ```python
> import cohere
>
> co = cohere.Client("YOUR_API_KEY")
> results = co.rerank(
>     query=question,
>     documents=documents,
>     top_n=3,
>     model="rerank-english-v2.0"
> )
> ```
>
> ê°œì¸ ê²½í—˜:
> - í”„ë¡œí† íƒ€ì…: LLM í‰ê°€ (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
> - í”„ë¡œë•ì…˜: Reranker ëª¨ë¸ (80% ì •í™•ë„, 10ë°° ë¹ ë¦„)
> - ëŒ€ê·œëª¨: Cohere Rerank (95% ì •í™•ë„, ì¤‘ê°„ ì†ë„)

---

## 5. Corrective RAG: ì§ˆë¬¸ ì¬ì‘ì„±ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ

### 5.1 Corrective RAG ê°œë…

Corrective RAGëŠ” ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš¸ ë•Œ, ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì—¬ ë‹¤ì‹œ ê²€ìƒ‰í•œë‹¤.

ë¬¸ì œ ìƒí™©:
```
ì§ˆë¬¸: "ìƒì„±í˜•AI ê°€ìš°ìŠ¤ë¥¼ ë§Œë“  íšŒì‚¬ëŠ”?"
ê²€ìƒ‰: "ì‚¼ì„±ì „ì", "LGì „ì", "ë„¤ì´ë²„" (ëª¨í˜¸í•œ ê²°ê³¼)
í‰ê°€: ê´€ë ¨ì„± 0.3 (ë‚®ìŒ)
```

í•´ê²°:
```
ì§ˆë¬¸ ì¬ì‘ì„±: "ì‚¼ì„± ìƒì„±í˜•AI ê°€ìš°ìŠ¤ ê°œë°œì‚¬"
ì¬ê²€ìƒ‰: "ì‚¼ì„±ì „ìê°€ 2023ë…„ ê°œë°œí•œ ìƒì„±í˜•AI ê°€ìš°ìŠ¤..."
í‰ê°€: ê´€ë ¨ì„± 0.9 (ë†’ìŒ)
ë‹µë³€ ìƒì„±: "ì‚¼ì„±ì „ìì…ë‹ˆë‹¤."
```

### 5.2 Corrective RAG êµ¬í˜„

State ì •ì˜:
```python
class CorrectiveRAGState(TypedDict):
    question: str
    rewritten_question: str
    documents: list[str]
    relevance_score: float
    retry_count: int
    answer: str
```

ë…¸ë“œ ì •ì˜:

1. ì§ˆë¬¸ ì¬ì‘ì„± ë…¸ë“œ:
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

def rewrite_query_node(state: CorrectiveRAGState) -> CorrectiveRAGState:
    """ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±"""
    question = state["question"]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
        ê²€ìƒ‰ ì—”ì§„ì´ ì •í™•í•œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”.

ì›ë˜ ì§ˆë¬¸: {question}

ì¬ì‘ì„±ëœ ì§ˆë¬¸:"""
    )

    result = llm.invoke(prompt.format(question=question))

    return {
        "rewritten_question": result.content,
        "retry_count": state.get("retry_count", 0) + 1
    }
```

2. í‰ê°€ ì¡°ê±´ í•¨ìˆ˜:
```python
def evaluate_and_decide(state: CorrectiveRAGState) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    relevance = state.get("relevance_score", 0)
    retry_count = state.get("retry_count", 0)

    if relevance >= 0.7:
        return "generate"  # ì¶©ë¶„íˆ ì¢‹ìŒ â†’ ë‹µë³€ ìƒì„±
    elif retry_count >= 3:
        return "give_up"   # 3ë²ˆ ì¬ì‹œë„ í›„ í¬ê¸°
    else:
        return "rewrite"   # ì§ˆë¬¸ ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰
```

ì „ì²´ ê·¸ë˜í”„:
```python
from langgraph.graph import StateGraph, END

graph = StateGraph(CorrectiveRAGState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_node)
graph.add_node("evaluate", evaluate_relevance_simple)
graph.add_node("rewrite", rewrite_query_node)
graph.add_node("generate", generate_answer_node)

# Edge ì¶”ê°€
graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "evaluate")

# Conditional Edge: í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°
graph.add_conditional_edges(
    "evaluate",
    evaluate_and_decide,
    {
        "generate": "generate",
        "rewrite": "rewrite",
        "give_up": END
    }
)

# ì¬ì‘ì„± í›„ ë‹¤ì‹œ ê²€ìƒ‰ (ìˆœí™˜!)
graph.add_edge("rewrite", "retrieve")
graph.add_edge("generate", END)

app = graph.compile()
```

íë¦„ë„:
```
retrieve â†’ evaluate â†’ (ê´€ë ¨ì„± ë†’ìŒ) â†’ generate â†’ END
              â†“              â†‘
         (ê´€ë ¨ì„± ë‚®ìŒ)          â”‚
              â†“              â”‚
           rewrite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (ì§ˆë¬¸ ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰)
```

> âš ï¸ ì£¼ì˜ì‚¬í•­: ë¬´í•œ ë£¨í”„ ë°©ì§€
>
> Corrective RAGëŠ” ìˆœí™˜ êµ¬ì¡°ë¼ ë¬´í•œ ë£¨í”„ ìœ„í—˜ì´ ìˆë‹¤:
>
> ë¬¸ì œ:
> ```python
> # retry_count ì²´í¬ ì—†ìŒ
> def evaluate(state):
>     if state["relevance"] < 0.7:
>         return "rewrite"  # ê³„ì† ì¬ì‹œë„ â†’ ë¬´í•œ ë£¨í”„!
> ```
>
> í•´ê²° ë°©ë²•:
>
> 1. ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ:
> ```python
> MAX_RETRIES = 3
>
> def evaluate(state):
>     if state.get("retry_count", 0) >= MAX_RETRIES:
>         return "give_up"
>     if state["relevance"] < 0.7:
>         return "rewrite"
>     return "generate"
> ```
>
> 2. recursion_limit ì„¤ì •:
> ```python
> result = app.invoke(
>     {"question": "..."},
>     config={"recursion_limit": 10}  # ìµœëŒ€ 10ë²ˆ ë…¸ë“œ ì‹¤í–‰
> )
> ```
>
> 3. íƒ€ì„ì•„ì›ƒ ì„¤ì •:
> ```python
> import signal
>
> def timeout_handler(signum, frame):
>     raise TimeoutError("ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
>
> signal.signal(signal.SIGALRM, timeout_handler)
> signal.alarm(30)  # 30ì´ˆ ì œí•œ
>
> try:
>     result = app.invoke({"question": "..."})
> finally:
>     signal.alarm(0)
> ```

---

## 6. Web Search RAG: ì™¸ë¶€ ê²€ìƒ‰ í†µí•©

### 6.1 Web Search RAG ê°œë…

ë‚´ë¶€ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ì„ ë•Œ, ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„í•œë‹¤.

ì‹œë‚˜ë¦¬ì˜¤:
```
ì§ˆë¬¸: "ìƒì„±í˜•AI ê°€ìš°ìŠ¤ë¥¼ ë§Œë“  íšŒì‚¬ì˜ 2023ë…„ ë§¤ì¶œì€?"

1ë‹¨ê³„: ë‚´ë¶€ PDF ê²€ìƒ‰
ê²€ìƒ‰ ê²°ê³¼: "ìƒì„±í˜•AI ê°€ìš°ìŠ¤ëŠ” ì‚¼ì„±ì „ì ì œí’ˆ"
í‰ê°€: ë§¤ì¶œ ì •ë³´ ì—†ìŒ (not_grounded)

2ë‹¨ê³„: ì§ˆë¬¸ ì¬ì‘ì„±
ì¬ì‘ì„±: "ì‚¼ì„±ì „ì 2023ë…„ ë§¤ì¶œ"

3ë‹¨ê³„: ì›¹ ê²€ìƒ‰
ê²€ìƒ‰ ê²°ê³¼: "ì‚¼ì„±ì „ì 2023ë…„ ë§¤ì¶œ 258.94ì¡°ì›" (ë„¤ì´ë²„ ë‰´ìŠ¤)

4ë‹¨ê³„: ìµœì¢… ë‹µë³€
"ì‚¼ì„±ì „ìì˜ 2023ë…„ ë§¤ì¶œì€ 258.94ì¡°ì›ì…ë‹ˆë‹¤."
```

### 6.2 Web Search RAG êµ¬í˜„

State ì •ì˜:
```python
class WebSearchRAGState(TypedDict):
    question: str
    rewritten_question: str
    documents: list[str]
    web_results: list[str]
    is_grounded: bool  # ë¬¸ì„œì— ë‹µì´ ìˆëŠ”ì§€
    answer: str
```

ë…¸ë“œ ì •ì˜:

1. Grounding í‰ê°€ ë…¸ë“œ:
```python
def evaluate_grounding_node(state: WebSearchRAGState) -> WebSearchRAGState:
    """ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€"""
    question = state["question"]
    documents = state["documents"]

    # LLMìœ¼ë¡œ í‰ê°€
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¬¸ì„œì— ì§ˆë¬¸ì˜ ë‹µì´ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”?

ë¬¸ì„œ:
{documents}

ì§ˆë¬¸: {question}

ë‹µ: yes ë˜ëŠ” noë¡œë§Œ ë‹µí•˜ì„¸ìš”."""
    )

    result = llm.invoke(prompt.format(
        documents="\n\n".join(documents),
        question=question
    ))

    is_grounded = "yes" in result.content.lower()

    return {"is_grounded": is_grounded}
```

2. ì›¹ ê²€ìƒ‰ ë…¸ë“œ:
```python
from langchain_community.tools import DuckDuckGoSearchRun

def web_search_node(state: WebSearchRAGState) -> WebSearchRAGState:
    """ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰"""
    query = state.get("rewritten_question", state["question"])

    # DuckDuckGo ê²€ìƒ‰
    search = DuckDuckGoSearchRun()
    results = search.run(query)

    return {"web_results": [results]}
```

3. ì¡°ê±´ í•¨ìˆ˜:
```python
def decide_next_step(state: WebSearchRAGState) -> str:
    """ë¬¸ì„œ ì¶©ë¶„ì„± íŒë‹¨"""
    if state["is_grounded"]:
        return "generate_from_docs"  # ë¬¸ì„œë§Œìœ¼ë¡œ ë‹µë³€
    else:
        return "rewrite_and_search_web"  # ì›¹ ê²€ìƒ‰ í•„ìš”
```

ì „ì²´ ê·¸ë˜í”„:
```python
graph = StateGraph(WebSearchRAGState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_node)
graph.add_node("evaluate_grounding", evaluate_grounding_node)
graph.add_node("rewrite", rewrite_query_node)
graph.add_node("web_search", web_search_node)
graph.add_node("generate_from_docs", generate_answer_node)
graph.add_node("generate_from_web", generate_answer_from_web_node)

# Edge
graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "evaluate_grounding")

# Conditional Edge
graph.add_conditional_edges(
    "evaluate_grounding",
    decide_next_step,
    {
        "generate_from_docs": "generate_from_docs",
        "rewrite_and_search_web": "rewrite"
    }
)

graph.add_edge("rewrite", "web_search")
graph.add_edge("web_search", "generate_from_web")
graph.add_edge("generate_from_docs", END)
graph.add_edge("generate_from_web", END)

app = graph.compile()
```

íë¦„ë„:
```
retrieve â†’ evaluate_grounding
              â”œâ”€ (grounded) â†’ generate_from_docs â†’ END
              â””â”€ (not grounded) â†’ rewrite â†’ web_search â†’ generate_from_web â†’ END
```

> ğŸ’¡ ì‹¤ë¬´ íŒ: ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„ íƒ
>
> | ë„êµ¬ | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ì‚¬ìš©ì²˜ |
> |------|------|------|-------------|
> | DuckDuckGoSearchRun | ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš” | ê²€ìƒ‰ í’ˆì§ˆ ë‚®ìŒ, ëŠë¦¼ | í”„ë¡œí† íƒ€ì… |
> | Google Search API | ê²€ìƒ‰ í’ˆì§ˆ ìµœê³  | ìœ ë£Œ ($5/1K ì¿¼ë¦¬) | í”„ë¡œë•ì…˜ (ì˜ˆì‚° ìˆìŒ) |
> | Tavily API | AI ê²€ìƒ‰ íŠ¹í™”, êµ¬ì¡°í™”ëœ ê²°ê³¼ | ìœ ë£Œ ($0.01/ê²€ìƒ‰) | RAG ì‹œìŠ¤í…œ |
> | SerpAPI | ë‹¤ì–‘í•œ ê²€ìƒ‰ì—”ì§„ ì§€ì› | ìœ ë£Œ ($50/5K ì¿¼ë¦¬) | ì¢…í•© ê²€ìƒ‰ |
> | Bing Search API | Microsoft ìƒíƒœê³„ í†µí•© | ìœ ë£Œ ($3/1K ì¿¼ë¦¬) | Azure ì‚¬ìš©ì |
>
> ê°œì¸ ê²½í—˜:
> - ì´ˆê¸° ê°œë°œ: DuckDuckGo (ë¬´ë£Œ)
> - MVP: Tavily (AI íŠ¹í™”, ê²°ê³¼ í’ˆì§ˆ ì¢‹ìŒ)
> - ëŒ€ê·œëª¨: Google Search API (ì‹ ë¢°ë„ ìµœê³ )
>
> Tavily ì‚¬ìš© ì˜ˆì‹œ:
> ```python
> from langchain_community.tools.tavily_search import TavilySearchResults
>
> search = TavilySearchResults(max_results=3)
> results = search.invoke("ì‚¼ì„±ì „ì 2023 ë§¤ì¶œ")
>
> # êµ¬ì¡°í™”ëœ ê²°ê³¼
> for result in results:
>     print(result["title"])
>     print(result["url"])
>     print(result["content"])
> ```

---

## 7. ì‹¤ì „ í”„ë¡œì íŠ¸: í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ

### 7.1 í”„ë¡œì íŠ¸ ê°œìš”

ë‹¤ìŒ ê¸°ëŠ¥ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” í”„ë¡œë•ì…˜ê¸‰ RAG ì‹œìŠ¤í…œ:

- Self-RAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
- Corrective RAG: ì§ˆë¬¸ ì¬ì‘ì„±
- Web Search: ì›¹ ê²€ìƒ‰ í†µí•©
- Human-in-the-Loop: ì¤‘ìš” íŒë‹¨ ì‹œ ì‚¬ìš©ì í™•ì¸

### 7.2 ì „ì²´ ì½”ë“œ

```python
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import DuckDuckGoSearchRun
import operator

# ========== State ì •ì˜ ==========
class HybridRAGState(TypedDict):
    question: str
    rewritten_question: str
    documents: Annotated[list[str], operator.add]
    web_results: list[str]
    relevance_score: float
    is_grounded: bool
    retry_count: int
    answer: str

# ========== ë…¸ë“œ ì •ì˜ ==========
def retrieve_node(state: HybridRAGState):
    """1. ë²¡í„° ì €ì¥ì†Œì—ì„œ ë¬¸ì„œ ê²€ìƒ‰"""
    question = state.get("rewritten_question") or state["question"]

    vectorstore = FAISS.load_local("./db", OpenAIEmbeddings())
    docs = vectorstore.similarity_search(question, k=3)

    return {"documents": [doc.page_content for doc in docs]}

def evaluate_relevance_node(state: HybridRAGState):
    """2. ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    question = state["question"]
    documents = state["documents"]

    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— í¬í•¨ë˜ëŠ”ì§€
    keywords = question.lower().split()
    doc_text = " ".join(documents).lower()

    matches = sum(1 for kw in keywords if kw in doc_text)
    relevance_score = matches / len(keywords) if keywords else 0

    return {"relevance_score": relevance_score}

def rewrite_query_node(state: HybridRAGState):
    """3. ì§ˆë¬¸ ì¬ì‘ì„±"""
    question = state["question"]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
    prompt = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”: {question}"
    )

    result = llm.invoke(prompt.format(question=question))

    return {
        "rewritten_question": result.content,
        "retry_count": state.get("retry_count", 0) + 1
    }

def evaluate_grounding_node(state: HybridRAGState):
    """4. ë¬¸ì„œì— ë‹µì´ ìˆëŠ”ì§€ í‰ê°€"""
    documents = state["documents"]
    question = state["question"]

    # LLM í‰ê°€
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template(
        """ë¬¸ì„œì— ì§ˆë¬¸ì˜ ë‹µì´ ìˆë‚˜ìš”?

ë¬¸ì„œ: {documents}
ì§ˆë¬¸: {question}

ë‹µ: yes ë˜ëŠ” no"""
    )

    result = llm.invoke(prompt.format(
        documents="\n".join(documents[:500]),  # ìµœëŒ€ 500ì
        question=question
    ))

    is_grounded = "yes" in result.content.lower()

    return {"is_grounded": is_grounded}

def web_search_node(state: HybridRAGState):
    """5. ì›¹ ê²€ìƒ‰"""
    query = state.get("rewritten_question") or state["question"]

    search = DuckDuckGoSearchRun()
    results = search.run(query)

    return {"web_results": [results]}

def generate_answer_node(state: HybridRAGState):
    """6. ìµœì¢… ë‹µë³€ ìƒì„±"""
    question = state["question"]
    documents = state["documents"]
    web_results = state.get("web_results", [])

    # ëª¨ë“  ì •ë³´ í†µí•©
    context = "\n\n".join(documents + web_results)

    llm = ChatOpenAI(model="gpt-4o-mini")
    prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""
    )

    result = llm.invoke(prompt.format(context=context, question=question))

    return {"answer": result.content}

# ========== ì¡°ê±´ í•¨ìˆ˜ ==========
def decide_relevance(state: HybridRAGState) -> str:
    """ë¬¸ì„œ ê´€ë ¨ì„± íŒë‹¨"""
    relevance = state["relevance_score"]
    retry_count = state.get("retry_count", 0)

    if relevance >= 0.5:
        return "evaluate_grounding"
    elif retry_count >= 2:
        return "web_search"
    else:
        return "rewrite"

def decide_grounding(state: HybridRAGState) -> str:
    """ë¬¸ì„œ ì¶©ë¶„ì„± íŒë‹¨"""
    if state["is_grounded"]:
        return "generate"
    else:
        return "rewrite_for_web"

# ========== ê·¸ë˜í”„ êµ¬ì„± ==========
graph = StateGraph(HybridRAGState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_node)
graph.add_node("evaluate_relevance", evaluate_relevance_node)
graph.add_node("rewrite", rewrite_query_node)
graph.add_node("evaluate_grounding", evaluate_grounding_node)
graph.add_node("web_search", web_search_node)
graph.add_node("generate", generate_answer_node)

# Entry Point
graph.set_entry_point("retrieve")

# Edge
graph.add_edge("retrieve", "evaluate_relevance")

# Conditional Edge 1: ê´€ë ¨ì„± íŒë‹¨
graph.add_conditional_edges(
    "evaluate_relevance",
    decide_relevance,
    {
        "evaluate_grounding": "evaluate_grounding",
        "rewrite": "rewrite",
        "web_search": "web_search"
    }
)

# ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰
graph.add_edge("rewrite", "retrieve")

# Conditional Edge 2: Grounding íŒë‹¨
graph.add_conditional_edges(
    "evaluate_grounding",
    decide_grounding,
    {
        "generate": "generate",
        "rewrite_for_web": "rewrite"
    }
)

# ì›¹ ê²€ìƒ‰ í›„ ë‹µë³€
graph.add_edge("web_search", "generate")
graph.add_edge("generate", END)

# ì»´íŒŒì¼
memory = MemorySaver()
app = graph.compile(checkpointer=memory)

# ========== ì‹¤í–‰ ==========
if __name__ == "__main__":
    result = app.invoke(
        {"question": "LangGraphì˜ ì£¼ìš” ì¥ì  3ê°€ì§€ëŠ”?"},
        config={
            "configurable": {"thread_id": "demo-1"},
            "recursion_limit": 15
        }
    )

    print("\n=== ìµœì¢… ë‹µë³€ ===")
    print(result["answer"])

    print("\n=== ì‹¤í–‰ í†µê³„ ===")
    print(f"ì¬ì‹œë„ íšŸìˆ˜: {result.get('retry_count', 0)}")
    print(f"ê´€ë ¨ì„± ì ìˆ˜: {result.get('relevance_score', 0):.2f}")
    print(f"ì›¹ ê²€ìƒ‰ ì‚¬ìš©: {'ì˜ˆ' if result.get('web_results') else 'ì•„ë‹ˆì˜¤'}")
```

### 7.3 ì‹¤í–‰ íë¦„ ì˜ˆì‹œ

Case 1: ì´ìƒì ì¸ ê²½ìš°
```
retrieve â†’ evaluate_relevance (0.8) â†’ evaluate_grounding (yes) â†’ generate â†’ END
```

Case 2: ì¬ì‘ì„± í•„ìš”
```
retrieve â†’ evaluate_relevance (0.3) â†’ rewrite â†’ retrieve â†’ evaluate_relevance (0.7)
        â†’ evaluate_grounding (yes) â†’ generate â†’ END
```

Case 3: ì›¹ ê²€ìƒ‰ í•„ìš”
```
retrieve â†’ evaluate_relevance (0.7) â†’ evaluate_grounding (no) â†’ rewrite
        â†’ retrieve â†’ evaluate_relevance (0.4) â†’ web_search â†’ generate â†’ END
```

> í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸
>
> 1. ì—ëŸ¬ ì²˜ë¦¬
> ```python
> def safe_node(func):
>     def wrapper(state):
>         try:
>             return func(state)
>         except Exception as e:
>             logging.error(f"Node {func.__name__} failed: {e}")
>             return {"error": str(e)}
>     return wrapper
>
> @safe_node
> def retrieve_node(state):
>     ...
> ```
>
> 2. íƒ€ì„ì•„ì›ƒ ì„¤ì •
> ```python
> from timeout_decorator import timeout
>
> @timeout(10)  # 10ì´ˆ ì œí•œ
> def web_search_node(state):
>     ...
> ```
>
> 3. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
> ```python
> import logging
>
> logging.basicConfig(level=logging.INFO)
>
> def retrieve_node(state):
>     logging.info(f"Retrieving for: {state['question']}")
>     # ... ë¡œì§ ...
>     logging.info(f"Found {len(docs)} documents")
> ```
>
> 4. ë¹„ìš© ì¶”ì 
> ```python
> from langchain.callbacks import get_openai_callback
>
> with get_openai_callback() as cb:
>     result = app.invoke({"question": "..."})
>     print(f"ë¹„ìš©: ${cb.total_cost:.4f}")
>     print(f"í† í°: {cb.total_tokens}")
> ```
>
> 5. A/B í…ŒìŠ¤íŒ…
> - ê¸°ì¡´ RAG vs LangGraph ì„±ëŠ¥ ë¹„êµ
> - ì •í™•ë„, ì‘ë‹µ ì‹œê°„, ë¹„ìš© ì¸¡ì •
> - ì‚¬ìš©ì ë§Œì¡±ë„ ì¡°ì‚¬

---

## 8. ê³ ê¸‰ ì£¼ì œ

### 8.1 Human-in-the-Loop

ì¤‘ìš”í•œ íŒë‹¨ì´ í•„ìš”í•œ ê²½ìš°, ì‚¬ëŒì´ ê°œì…í•  ìˆ˜ ìˆë‹¤.

```python
def human_approval_node(state: HybridRAGState):
    """ì‚¬ìš©ì í™•ì¸ ìš”ì²­"""
    answer = state["answer"]

    print(f"\nìƒì„±ëœ ë‹µë³€:\n{answer}\n")
    approval = input("ì´ ë‹µë³€ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")

    if approval.lower() == 'y':
        return {"approved": True}
    else:
        return {"approved": False, "retry_count": state["retry_count"] + 1}

# Conditional Edge
def check_approval(state):
    if state.get("approved"):
        return "end"
    else:
        return "regenerate"

graph.add_node("human_approval", human_approval_node)
graph.add_edge("generate", "human_approval")
graph.add_conditional_edges(
    "human_approval",
    check_approval,
    {
        "end": END,
        "regenerate": "rewrite"
    }
)
```

### 8.2 Multi-Agent í˜‘ì—…

ì—¬ëŸ¬ Agentê°€ ì—­í•  ë¶„ë‹´í•˜ì—¬ ì‘ì—…í•œë‹¤.

```python
class MultiAgentState(TypedDict):
    question: str
    research_result: str  # Researcher Agent
    review_result: str    # Reviewer Agent
    final_answer: str     # Writer Agent

# Researcher: ì •ë³´ ìˆ˜ì§‘
def researcher_node(state):
    # RAG ê²€ìƒ‰
    ...
    return {"research_result": result}

# Reviewer: ê²€ì¦
def reviewer_node(state):
    # íŒ©íŠ¸ ì²´í¬
    ...
    return {"review_result": "approved"}

# Writer: ë‹µë³€ ì‘ì„±
def writer_node(state):
    # ìµœì¢… ë‹µë³€
    ...
    return {"final_answer": answer}

graph.add_node("researcher", researcher_node)
graph.add_node("reviewer", reviewer_node)
graph.add_node("writer", writer_node)

graph.set_entry_point("researcher")
graph.add_edge("researcher", "reviewer")
graph.add_edge("reviewer", "writer")
graph.add_edge("writer", END)
```

### 8.3 ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥

ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì¤€ë‹¤.

```python
# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
for step in app.stream({"question": "LangGraphë€?"}):
    node_name = list(step.keys())[0]
    node_output = step[node_name]

    print(f"\n[{node_name}] ì‹¤í–‰ ì¤‘...")

    if "documents" in node_output:
        print(f"  â†’ ë¬¸ì„œ {len(node_output['documents'])}ê°œ ê²€ìƒ‰")

    if "relevance_score" in node_output:
        print(f"  â†’ ê´€ë ¨ì„±: {node_output['relevance_score']:.2f}")

    if "answer" in node_output:
        print(f"  â†’ ë‹µë³€: {node_output['answer'][:100]}...")
```

### 8.4 ì„±ëŠ¥ ìµœì í™”

1. ë³‘ë ¬ ì²˜ë¦¬:
```python
# ì—¬ëŸ¬ ë…¸ë“œë¥¼ ë™ì‹œì— ì‹¤í–‰
graph.add_node("retrieve_pdf", retrieve_pdf_node)
graph.add_node("retrieve_web", retrieve_web_node)

# ë‘ ë…¸ë“œ ë³‘ë ¬ ì‹¤í–‰
graph.set_entry_point("retrieve_pdf")
graph.set_entry_point("retrieve_web")  # ë™ì‹œ ì‹œì‘

# ê²°ê³¼ ë³‘í•©
graph.add_node("merge", merge_results_node)
graph.add_edge("retrieve_pdf", "merge")
graph.add_edge("retrieve_web", "merge")
```

2. ìºì‹±:
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def retrieve_cached(question: str):
    """ë™ì¼ ì§ˆë¬¸ì€ ìºì‹œì—ì„œ ë°˜í™˜"""
    return vectorstore.search(question)
```

---

## 9. ë””ë²„ê¹… ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1 ì¼ë°˜ì ì¸ ë¬¸ì œ

ë¬¸ì œ 1: ë¬´í•œ ë£¨í”„
```
ì¦ìƒ: í”„ë¡œê·¸ë¨ì´ ë©ˆì¶”ì§€ ì•ŠìŒ
ì›ì¸: retry_count ì²´í¬ ëˆ„ë½
í•´ê²°: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì •
```

ë¬¸ì œ 2: State í‚¤ ì˜¤ë¥˜
```python
# ì—ëŸ¬: KeyError: 'rewritten_question'
# ì›ì¸: ë…¸ë“œì—ì„œ ìƒì„±í•˜ì§€ ì•Šì€ í‚¤ ì°¸ì¡°

# í•´ê²°
def safe_get(state, key, default=None):
    return state.get(key, default)

question = safe_get(state, "rewritten_question", state["question"])
```

ë¬¸ì œ 3: LLM í˜¸ì¶œ ì‹¤íŒ¨
```python
# Rate limit, timeout ë“±
def llm_call_with_retry(prompt, max_retries=3):
    for i in range(max_retries):
        try:
            return llm.invoke(prompt)
        except Exception as e:
            if i == max_retries - 1:
                raise
            time.sleep(2  i)  # Exponential backoff
```

### 9.2 ë¡œê·¸ ë¶„ì„

```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('langgraph.log'),
        logging.StreamHandler()
    ]
)

def retrieve_node(state):
    logging.debug(f"State input: {state}")
    # ... ë¡œì§ ...
    logging.info(f"Retrieved {len(docs)} documents")
    logging.debug(f"State output: {updated_state}")
    return updated_state
```

---

## ì •ë¦¬

LangGraphëŠ” ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§í•˜ì—¬, ìœ ì—°í•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

í•µì‹¬ ê°œë…:
- State: ë…¸ë“œ ê°„ ê³µìœ  ë©”ëª¨ë¦¬
- Node: ì‘ì—… ìˆ˜í–‰ í•¨ìˆ˜
- Edge: ë…¸ë“œ ì—°ê²°
- Conditional Edge: ì¡°ê±´ë¶€ ë¶„ê¸°

ê³ ê¸‰ íŒ¨í„´:
- Self-RAG: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
- Corrective RAG: ì§ˆë¬¸ ì¬ì‘ì„±
- Web Search RAG: ì›¹ ê²€ìƒ‰ í†µí•©
- Human-in-the-Loop: ì‚¬ëŒ ê°œì…

ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­:
1. ë¬´í•œ ë£¨í”„ ë°©ì§€ (retry_count, recursion_limit)
2. ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
3. ë¹„ìš© ì¶”ì  ë° ìµœì í™”
4. A/B í…ŒìŠ¤íŒ…ìœ¼ë¡œ íš¨ê³¼ ê²€ì¦

LangGraphëŠ” í”„ë¡œë•ì…˜ê¸‰ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì— í•„ìˆ˜ ë„êµ¬ë‹¤. ê¸°ë³¸ RAGë¡œ ì‹œì‘í•˜ì—¬, ì ì§„ì ìœ¼ë¡œ Self-RAG, Corrective RAGë¥¼ ì¶”ê°€í•˜ë©° ê°œì„ í•˜ì.

ì°¸ê³ 
LangGraphëŠ” LangChain ìƒíƒœê³„ì˜ ìµœì‹  ë„êµ¬ë¡œ, ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸ëœë‹¤. ê³µì‹ ë¬¸ì„œ(https://langchain-ai.github.io/langgraph/)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì.
